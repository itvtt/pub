{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree(CART : Classification and Regression Tree)\n",
    "> **`Decision Tree`** 모델은 **예측/분류가 모두 가능**한 **지도학습** 머신러닝 모델이다.   \n",
    "스무고개 게임을 하듯 여러 개의 가정을 데이터에 반영하고 이를 바탕으로 결정경계(decision boundary)를 생성  \n",
    "모델 예측 및 분류 결과에 따른 해석이 굉장히 용이하여 **모델 해석이 필요한 문제에 사용**한다.ex)신용평가, 모델분류  \n",
    "최근에는 `Decision Tree`모델을 베이스로 한 부스팅 트리 모델(**`Xgboost`**, **`LightGBM`**, **`Catboost`**)등으로 데이터분석 대회 수상을 하면서 실무 적용 케이스가 많아졌다.\n",
    "\n",
    "### 모델구조\n",
    "> 뿌리 노드(root node) : 최상위 노드, 모든 샘플 포함  \n",
    "잎 노드(leaf node) : 최하위 노드, 여기에 속한 샘플이 어떤 클래스인지 결정 됨  \n",
    "노드(node) : 뿌리 노드와 잎 노드 사이에 있는 노드  \n",
    "가지(branch) : 노드를 나누는 기준  \n",
    "깊이(depth) : 뿌리 노드와 잎 노드 까지의 노드 갯수\n",
    "\n",
    "<img src=\"./image/27.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델학습\n",
    "#### 불순도\n",
    "> `Decision Tree` 모델을 학습시키는 방법  \n",
    "정보화 이론에서 사용하는 Gini 계수와 엔트로피를 사용한다.  \n",
    "불순도가 0.5에 가까수록 불순도가 높고 0 혹은 1에 가까울 수록 순도가 높다.  \n",
    "즉, 한 노드의 불순도가 가능한 많이 떨어지도록(순도가 올라가도록) 노드를 나눈다.\n",
    "\n",
    "$$ Gini = 1 - \\sum_1^n{(p_i)^2} $$\n",
    "\n",
    "$$ Entropy = - \\sum_1^n{p_iln(p_i)} $$\n",
    "\n",
    "#### Gini index\n",
    "위 예시에서 뿌리 노드 기준 지니계수 계산법  \n",
    "class1 : 삼각형  \n",
    "class2 : 동그라미  \n",
    ">X < 0\n",
    ">> True = class1 3개, class2 4개  \n",
    "$1 - ({3 \\over 3+4})^2 - ({4 \\over 3+4})^2 = 0.48$  \n",
    "False = class1 4개, class2 3개  \n",
    "$1 - ({4 \\over 4+3})^2 - ({3 \\over 4+3})^2 = 0.48$  \n",
    "total Gini 계수  \n",
    "$1 - ({7 \\over 7+7})0.48 - ({7 \\over 7+7})0.48 = 0.52$\n",
    "\n",
    "위 예시에서 잎 노드 기준 지니계수 계산법  \n",
    "class1 : 삼각형  \n",
    "class2 : 동그라미  \n",
    ">Y < 1\n",
    ">> True = class1 3개, class2 0개  \n",
    "$1 - ({3 \\over 3})^2 - ({0 \\over 3})^2 = 0$  \n",
    "False = class1 0개, class2 4개  \n",
    "$1 - ({0 \\over 4})^2 - ({4 \\over 4})^2 = 0$  \n",
    "total Gini 계수  \n",
    "$1 - ({3 \\over 3+4})0 - ({4 \\over 3+4})0 = 1$\n",
    "\n",
    "위의 예시에서 계산한 total Gini 계수가 곧 Decision tree 모델의 비용함수가 된다.  \n",
    "이를 바탕으로 더 나은 선택을 하게 되는 결정경계를 생성하는 방법으로 데이터를 학습하는데 이를 greedy 알고리즘이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree classifier 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.690669Z",
     "start_time": "2023-03-30T08:17:43.687273Z"
    }
   },
   "outputs": [],
   "source": [
    "# 필요모듈 import \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.693997Z",
     "start_time": "2023-03-30T08:17:43.692302Z"
    }
   },
   "outputs": [],
   "source": [
    "# iris 데이터로드\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.697306Z",
     "start_time": "2023-03-30T08:17:43.695566Z"
    }
   },
   "outputs": [],
   "source": [
    "# 로딩 데이터 확인\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.700870Z",
     "start_time": "2023-03-30T08:17:43.699445Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "X = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.703605Z",
     "start_time": "2023-03-30T08:17:43.701975Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.706407Z",
     "start_time": "2023-03-30T08:17:43.704850Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "\n",
    "dtc =  DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.709489Z",
     "start_time": "2023-03-30T08:17:43.707708Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "dtc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dtc.predict(X_test)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 확률값 확인\n",
    "# CODE HERE\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.715033Z",
     "start_time": "2023-03-30T08:17:43.713385Z"
    }
   },
   "outputs": [],
   "source": [
    "# test셋 분류 결과 확인\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.712260Z",
     "start_time": "2023-03-30T08:17:43.710602Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 평가지표 출력\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred, target_names=iris['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 해석을 위한 시각화 방법\n",
    "#### feature importance\n",
    "트리 기반 모델은 트리를 분기하는 과정에서 어떤 변수가 모델을 학습하는데 중요한지 살펴볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.721885Z",
     "start_time": "2023-03-30T08:17:43.720414Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature importance 시각화\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,pred))\n",
    "# print(precision_score(y_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(iris['feature_names'], dtc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.724423Z",
     "start_time": "2023-03-30T08:17:43.722760Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 시각화\n",
    "plt.figure(figsize=(10,8))\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(dtc, feature_names=iris['feature_names'],class_names=iris['target_names'],filled=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.scatterplot(data=X_train, x='petal length (cm)',  y='petal width (cm)', hue=y_train) # 변수 중요도에 따라 중요 변수 2개를 각 축에 위치시켜서 산점도 출력\n",
    "plt.vlines(2.45, 0, 2.5) # plot_tree 첫 뎁스 기준값에 따른 영역 설정\n",
    "plt.hlines(1.55, 1, 7) # 두번째 뎁스 기준값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가지치기 (pruning)\n",
    ">`Decision Tree`모델은 모든 **잎 노드의 불순도가 0이 되는 순간까지 모델을 성장**시키면서 크기를 키워나간다.  \n",
    "순수 노드로만 이루어진 트리 모델은 훈련 데이터를 100% 정확도로 맞출 수 있다.  \n",
    "이러한 특성 때문에 트리 모델은 **과적합에 취약**하다.  \n",
    "과적합 방지를 위해서는 **트리의 복잡도를 제어** 할 필요가 있다.\n",
    "\n",
    ">과적합 방지를 위한 모델링 파라메터  \n",
    ">> - **`max_depth`** : 트리의 최대 깊이  \n",
    "- `max_leaf_nodes` : 잎 노드의 최대개수  \n",
    "- `min_sample_leaf` : 잎 노드가 되기 위한 최소 샘플 갯수  \n",
    "- `min_sample_split` : 잎 노드가 분지 되기 위한 최소 샘플 갯수\n",
    "\n",
    "위의 iris 데이터는 3개의 클래스로 이루어진 데이터셋이지만 모델플로팅 결과 2뎁스의 노드에서 어느정도 데이터 구분이 되었습니다.  \n",
    "이를 기준으로 사후 가지치기를 진행 해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree regressor\n",
    "> `Decision Tree`모델은 알고리즘 특성으로 분류 및 예측 모델링에 모두 사용이 가능하다.  \n",
    "일반적으로 잎 노드에 속한 학습샘플의 값의 평균을 바탕으로 예측값을 결정한다.  \n",
    "회귀모델 평가 방법인 MSE를 각 노드에 속한 샘플에 적용하고 이를 최소화 시킨다.  \n",
    "\n",
    "<img src=\"./image/28.png\">\n",
    "<img src=\"./image/29.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressor 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.727123Z",
     "start_time": "2023-03-30T08:17:43.725585Z"
    }
   },
   "outputs": [],
   "source": [
    "# 보스턴 집값 데이터 로딩\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/boston.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.729344Z",
     "start_time": "2023-03-30T08:17:43.728103Z"
    }
   },
   "outputs": [],
   "source": [
    "# 타겟 데이터 분할\n",
    "\n",
    "y = df['y']\n",
    "X = df.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.731534Z",
     "start_time": "2023-03-30T08:17:43.730162Z"
    }
   },
   "outputs": [],
   "source": [
    "# 테스트 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.733831Z",
     "start_time": "2023-03-30T08:17:43.732394Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.736015Z",
     "start_time": "2023-03-30T08:17:43.734768Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "dtc = DecisionTreeRegressor(max_depth=6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.738657Z",
     "start_time": "2023-03-30T08:17:43.736697Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.740834Z",
     "start_time": "2023-03-30T08:17:43.739512Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 예측\n",
    "\n",
    "pred = dtc.predict(X_test)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.743067Z",
     "start_time": "2023-03-30T08:17:43.741579Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 평가지표 출력\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.745394Z",
     "start_time": "2023-03-30T08:17:43.744066Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature importance\n",
    "dtc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 시각화\n",
    "dtc.get_depth()\n",
    "plt.figure(figsize=(20,8))\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(dtc,filled=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T06:30:12.192008Z",
     "start_time": "2021-07-23T06:30:12.189331Z"
    }
   },
   "source": [
    "## Random Forest\n",
    ">**`Random forest`** 는 **`Decision Tree`** 모델의 **모형 결합(ensemble)방법론**  \n",
    "\n",
    "### ensemble(앙상블)\n",
    "> **복수의 예측 모형을 결합**하여 더 나은 성능의 예측을 하려는 시도이다.  \n",
    "단일 모형을 사용할 때 보다 **성능 분산이 감소**하고, 즉 **과적합을 방지**한다.  \n",
    "개별 모형이 성능이 안좋을 경우에는 결합 모형의 성능이 더 향상된다.  \n",
    "앙상블 방법론에는 **배깅**, **부스팅**이 있다.\n",
    "\n",
    "<img src=\"./image/30.gif\">\n",
    "\n",
    "#### bagging(배깅)\n",
    "> 개별 모델을 병렬로 구성하여 모델을 결합하는 방법론이다.  \n",
    "기존 학습데이터에서 **복원 추출**로 여러개의 sub sample 데이터셋을 만든 후 각 데이터셋을 병렬 구성 모델에 학습시켜 서로 다른 결과를 얻는다.  \n",
    "개별 모델의 결과값을 voting(투표법) 혹은 평균법을 사용하여 개별 모델 결과를 바탕으로 최종 추정치를 얻는다.  \n",
    "\n",
    "<img src=\"./image/31.png\">\n",
    "\n",
    "#### Random Forest Bootstrap Aggregating\n",
    "> **`Random forest`** 는 대표적인 배깅 방법론으로 weak model로 **`Decision Tree`** 를 사용한다.  \n",
    "배깅 사용 시 추가적으로 부트스트랩 방법론을 추가하여 모델 학습에 사용한다.  \n",
    "부트스트랩은 복원 추출 된 sub sample 데이터셋 생성 시 랜덤 샘플 및 feature를 선택하여 모델 학습에 사용한다.\n",
    "\n",
    "<img src=\"./image/32.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ramdom Forest 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보스턴 집값 데이터 로딩\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.747650Z",
     "start_time": "2023-03-30T08:17:43.746352Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과적합 방지를 위한 모델링 파라메터  \n",
    "> - **n_estimators** : 사용 할 트리 모델 갯수  \n",
    "- **max_depth** : 트리의 최대 깊이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['y']\n",
    "X = df.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.750263Z",
     "start_time": "2023-03-30T08:17:43.748807Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "rfr = RandomForestRegressor(max_depth=13, random_state=42)\n",
    "# 모델 학습\n",
    "rfr.fit(X_train, y_train)\n",
    "pred = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.756874Z",
     "start_time": "2023-03-30T08:17:43.755341Z"
    }
   },
   "outputs": [],
   "source": [
    "# 평가\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Tree\n",
    "> 배깅과 부스팅의 차이점은 학습을 위해 사용하는 개별모델을 병렬/직렬로 구성함에 있다.  \n",
    "배깅의 경우 sub sample에 따라 개별 모델을 모두 학습시키고 결과를 투표 혹은 평균을 내어 예측한다면  \n",
    "부스팅은 **개별 모델의 학습을 순차적**으로 시키며 이전 개별 모델의 결과 중 **오분류 된 데이터 혹은 오차에 가중치 부여**  \n",
    "초기에는 동일 가중치를 갖지만 각 학습 과정을 거치며 복원 추출 시 가중치의 분포/이전 round의 오차를 고려  \n",
    "\n",
    ">> 해당모델에는 `Adaboost`, `GBM`, `Xgboost`, `lightGBM`, `catboost`가 있다.\n",
    "\n",
    "### bagging 과 boosting\n",
    "<img src=\"./image/33.png\">\n",
    "\n",
    "### Adaptive booting(Adaboost)\n",
    "> a -> f 순서로 학습이 진행 되고 있다. 각 학습 단계(round)에서 오분류 된 데이터에 가중치를 부여하고  \n",
    "다음 라운드에서 가중치가 부여 된 데이터를 잘 맞추기 위한 개별모델이 학습 된다.  \n",
    "최종 모델은 개별 모델의 결과가 합쳐져서 최종 모델링이 된다.\n",
    "\n",
    "<img src=\"./image/34.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient boost\n",
    "이전 round 모델의 데이터별 오류를 학습하는 모델을 사용하여 점진적으로 총 모델링 오차를 줄이는 부스팅 방법\n",
    "\n",
    "$$y = h_0(x) + error_0 $$\n",
    "$$error_0 = h_1(x) + error_1 $$\n",
    "$$error_1 = h_2(x) + error_2 $$\n",
    "$$\\vdots$$\n",
    "$$y = h_0(x) + h_1(x) + h_2(x) + \\cdots + small error $$\n",
    "\n",
    "<img src=\"./image/35.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost\n",
    "> 머신러닝 알고리즘 대회인 kaggle, KDD cup등에서 우승을 한 팀들이 xgboost를 많이 활용한 것이 알려지면서 주목받음.  \n",
    "boosting 모델에서 오류를 학습하여 다음 round에 반영시키는 것은 gadient boosting과 큰 차이가 없음.  \n",
    "다만, 학습을 위한 비용함수에 규제화 식이 추가되어 모델이 과적합 되는 것을 방지함.  \n",
    "규제화를 통해 복잡한 모델에 패널티를 부여  \n",
    "\n",
    "$$obj^{(t)} = \\sum_1^{n} l(y_i, \\hat{y}_i^{(t)}) + \\sum_{i=1}^t \\Omega(f_i) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.759364Z",
     "start_time": "2023-03-30T08:17:43.757893Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 설치\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.767298Z",
     "start_time": "2023-03-30T08:17:43.765811Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.772216Z",
     "start_time": "2023-03-30T08:17:43.770845Z"
    }
   },
   "outputs": [],
   "source": [
    "# 보스턴 집값 데이터 로딩\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/boston.csv')\n",
    "\n",
    "y = df['y']\n",
    "X = df.drop('y', axis=1)\n",
    "\n",
    "# 테스트셋 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 훈련과 테스트셋에서의 레이블의 분포가 동일하게 하라. (stratify=label)\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.779565Z",
     "start_time": "2023-03-30T08:17:43.773091Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "\n",
    "'''\n",
    "xbgoost 주요 파라메터\n",
    "\n",
    "모델 파라메터\n",
    "verbosity : round 출력결과 0=무음, 1=경고, 2=정보, 3=디버그\n",
    "n_jobs : 병렬쓰레드 구성, 로컬컴퓨터 코어 x 4 최대값\n",
    "gpu_id : GPU 연산 처리 디바이스 설정\n",
    "random_state : 랜덤시드\n",
    "missing : 결측치 처리 np.nan을 디폴트로 사용\n",
    "\n",
    "트리 파라메터\n",
    "max_depth : 트리모델 최대 깊이\n",
    "max_leaves : 트리모델 최대 잎 노드 갯수, 0=무제한 지정\n",
    "grow_policy : 트리확장 방법 0=노드와 가장 가까운 노드 분할, 1=손실함수가 최소가 되는 지점에서 분할\n",
    "gamma : 트리모델의 잎 노드 분할을 만드는 데 필요한 최소 손실 감소.\n",
    "min_child_weight : 관측치에 대한 최소 가중치 값\n",
    "subsample : 부트스트랩 샘플 비율\n",
    "colsample_bytree : 부트스트랩 컬럼 비율\n",
    "reg_alpha : L1, lasso, 0\n",
    "reg_lambda : L2, ridge, 1\n",
    "\n",
    "부스팅 파라메터\n",
    "n_estimators : 부스팅 트리 갯수, round 횟수와 같은 수\n",
    "learning_rate : round별 학습률\n",
    "booster: 부스팅 트리 모델 선택\n",
    "    gbtree\n",
    "    gblinear\n",
    "objective : 목적함수 \n",
    "    reg : squarederror\n",
    "    binary : logistic\n",
    "    multi : softmax\n",
    "    multi : softprob\n",
    "eval_metric : 모델평가함수, 목적함수에 따라 지정되어 있음\n",
    "    rmse: root mean square error\n",
    "    error: Binary classification error rate (0.5 threshold)\n",
    "    merror: Multiclass classification error rate\n",
    "early_stopping_rounds : 학습 손실값 변동 없을 시 학습 종료 라운드 횟수 설정\n",
    "callbacks : 학습 중 설정 값 전달 API\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.781886Z",
     "start_time": "2023-03-30T08:17:43.780510Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "xg = XGBRegressor()\n",
    "xg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.784081Z",
     "start_time": "2023-03-30T08:17:43.782627Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 예측\n",
    "\n",
    "pred = xg.predict(X_test)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.786525Z",
     "start_time": "2023-03-30T08:17:43.785173Z"
    }
   },
   "outputs": [],
   "source": [
    "# 평가지표 출력\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.788987Z",
     "start_time": "2023-03-30T08:17:43.787599Z"
    }
   },
   "outputs": [],
   "source": [
    "# 변수 중요도 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라메터 서칭\n",
    "tree base 모델은 설정 가능한 파라메터의 조합에 따라 모델 예측력 차이가 큰 특징을 가지고 있습니다.  \n",
    "특히, Xgboost 모델의 경우 파라메터 설정에 따른 모델 예측력 차이가 굉장히 크기에 꼭 파라메터 서칭을 진행해주셔야 합니다.  \n",
    "간단한 문법을 통해 파라메터 서칭을 진행 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.791534Z",
     "start_time": "2023-03-30T08:17:43.789830Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.793527Z",
     "start_time": "2023-03-30T08:17:43.792281Z"
    }
   },
   "outputs": [],
   "source": [
    "# product 함수로 파라메터의 모든 조합 만들기\n",
    "# random forest 서칭이 필요한 파라미터 max_depth, n_estimators\n",
    "# 분류모델의 경우 max_depth를 얕게, n_estimators 늘림\n",
    "# 예측모델의 경우 max_depth를 깊게, n_estimators 기본\n",
    "\n",
    "depth = [5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "est = [80, 100, 300, 500, 700, 900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증셋 validation set 생성\n",
    "X_train3,X_val,y_train3,y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_param = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in list(product(depth, est)):\n",
    "    print(f'{param} 서칭중')\n",
    "    model = RandomForestRegressor(n_estimators=param[1], max_depth=param[0], random_state=42)\n",
    "    model.fit(X_train3, y_train3)\n",
    "    pred = model.predict(X_val)   #파라미터 서칭을 진행하며 사용하는 검증셋을 validation셋 사용\n",
    "    r2 = r2_score(y_val, pred)\n",
    "    if r2 > best_score : \n",
    "        print('스코어 갱신')\n",
    "        best_param = param \n",
    "        best_score = r2\n",
    "print('학습완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.796127Z",
     "start_time": "2023-03-30T08:17:43.794569Z"
    }
   },
   "outputs": [],
   "source": [
    "# 위 파라메터 조합을 반복문으로 순환하며 파라메터 서칭\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.798718Z",
     "start_time": "2023-03-30T08:17:43.797316Z"
    }
   },
   "outputs": [],
   "source": [
    "# 최적 r2값, 최적 파라메터 확인\n",
    "\n",
    "best_score, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.801048Z",
     "start_time": "2023-03-30T08:17:43.799470Z"
    }
   },
   "outputs": [],
   "source": [
    "# 최적 모델로 모델 다시 학습 및 평가\n",
    "best_model = RandomForestRegressor(n_estimators=best_param[1],max_depth=best_param[0],random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "best_pred = best_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가1 예측\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, best_pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, best_pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost : 서칭시 확인해야 하는 파라미터 조합 및 범위\n",
    "# max_depth : 분류 [3,5,7,9] 깊이를 얕게 [15, 17, 19, 21~] 깊이를 깊게\n",
    "# n_estimators : 분류 [100, 300, 500, 700] 예측 [50, 70, 100, 200]\n",
    "# learning_rate : [0.03, 0.01, 0.003, 0.001]\n",
    "# subsample : [0.6, 0.7, 0.8, 0.9]\n",
    "# col_sample_bytree : [0.6, 0.7, 0.8, 0.9] 부트스트랩 컬럼 비율\n",
    "# 과적합 발생 가능성이 있는 경우 아래 파라미터까지 서칭\n",
    "\n",
    "# reg_alpha : L1, lasso, [0,1,3,5,7,9]\n",
    "# reg_lambda : L2, ridge, [1,3,5,7,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn GridSearchCV\n",
    "sklearn 패키지에는 위의 파라메터 서칭 과정을 간편하게 진행 할 수 있도록 GridSearchCV 방법론을 제공합니다.  \n",
    "기존 파라메터 서칭과 함께 cross validation 과정을 추가하여 데이터 분할에 강건한 모델을 선택할 수 있도록 제작 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# 분류\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer['data']\n",
    "y = cancer['target']\n",
    "\n",
    "# 예측\n",
    "boston = pd.read_csv('./data/boston.csv')\n",
    "X2 = boston.drop('y', axis=1)\n",
    "y2 = boston['y']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train2,X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.803536Z",
     "start_time": "2023-03-30T08:17:43.802156Z"
    }
   },
   "outputs": [],
   "source": [
    "# 그리드 서치 import\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgbr = XGBRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth' : [15, 17, 19, 20],\n",
    "        'n_estimators' : [50,70,100,200],\n",
    "        'learning_rate' : [0.03,0.01],\n",
    "        'subsample' : [0.7,0.8,0.9],\n",
    "        'colsample_bytree' : [0.7, 0.8, 0.9]}\n",
    "\n",
    "\n",
    "# xgboost : 서칭시 확인해야 하는 파라미터 조합 및 범위\n",
    "# max_depth : 분류 [3,5,7,9] 깊이를 얕게 [15, 17, 19, 21~] 깊이를 깊게\n",
    "# n_estimators : 분류 [100, 300, 500, 700] 예측 [50, 70, 100, 200]\n",
    "# learning_rate : [0.03, 0.01, 0.003, 0.001]\n",
    "# subsample : [0.6, 0.7, 0.8, 0.9]\n",
    "# col_sample_bytree : [0.6, 0.7, 0.8, 0.9] 부트스트랩 컬럼 비율\n",
    "# 과적합 발생 가능성이 있는 경우 아래 파라미터까지 서칭\n",
    "\n",
    "# reg_alpha : L1, lasso, [0,1,3,5,7,9]\n",
    "# reg_lambda : L2, ridge, [1,3,5,7,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.806512Z",
     "start_time": "2023-03-30T08:17:43.804498Z"
    }
   },
   "outputs": [],
   "source": [
    "# 그리드 서치 실습\n",
    "grid = GridSearchCV(estimator=xgbr,\n",
    "                    param_grid=params,\n",
    "                    scoring='neg_root_mean_squared_error',\n",
    "                    n_jobs=-1,\n",
    "                    cv=5\n",
    "                    )\n",
    "'''\n",
    "estimator : 모델 딕셔너리\n",
    "param_grid : 파라메터 딕셔너리\n",
    "scoring=None : 평가방법\n",
    "n_jobs=None : 학습에 사용할 컴퓨터 코어 갯수\n",
    "verbose=0 : 리포트 형식 0, 1, 2\n",
    "\n",
    "scoring 참고\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.808587Z",
     "start_time": "2023-03-30T08:17:43.807186Z"
    }
   },
   "outputs": [],
   "source": [
    "# grid 학습\n",
    "\n",
    "grid.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.811030Z",
     "start_time": "2023-03-30T08:17:43.809500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 최적 모델 및 파라메터 확인\n",
    "grid_pred = grid.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(r2_score(y_test2, grid_pred))\n",
    "print(f'{mean_squared_error(y_test2, grid_pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "print(confusion_matrix(y_test, lr_pred))\n",
    "print(classification_report(y_test, lr_pred))\n",
    "print(recall_score(y_test, lr_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =abs(y_train -1)\n",
    "y_test = abs(y_test -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgbc = XGBClassifier(random_state=42)\n",
    "params2 = {'max_depth' : [3, 5, 7, 9],\n",
    "        'n_estimators' : [100,200],\n",
    "        'learning_rate' : [0.03, 0.01],\n",
    "        'subsample' : [0.7,0.8,0.9],\n",
    "        'colsample_bytree' : [0.7, 0.8, 0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2 = GridSearchCV(estimator=xgbc, param_grid=params2, scoring='recall', verbose=2, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2_pred = grid2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, grid2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kospi = pd.read_csv('./data/kospi.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = kospi.drop('코스피지수', axis=1)\n",
    "y = kospi['코스피지수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 훈련과 테스트셋에서의 레이블의 분포가 동일하게 하라. (stratify=label)\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "# 모델 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =  LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가1 예측\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kospi = pd.read_csv('./data/kospi.csv', encoding='cp949')\n",
    "X = kospi.drop('코스피지수', axis=1)\n",
    "y = kospi['코스피지수']\n",
    "# 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 훈련과 테스트셋에서의 레이블의 분포가 동일하게 하라. (stratify=label)\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.scatter(pred, y_test, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr =  LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "# 모델 예측\n",
    "pred = lr.predict(X_test)\n",
    "# 모델 평가1 예측\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "# 모델 학습\n",
    "xg = XGBRegressor()\n",
    "xg.fit(X_train, y_train)\n",
    "# 모델 예측\n",
    "pred = xg.predict(X_test)\n",
    "# 평가지표 출력\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# 모델 생성\n",
    "rfr = RandomForestRegressor(max_depth=16)\n",
    "# 모델 학습\n",
    "rfr.fit(X_train, y_train)\n",
    "pred = rfr.predict(X_test)\n",
    "# 평가\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# 모델 생성\n",
    "dtc = DecisionTreeRegressor(max_depth=11)\n",
    "# 모델 학습\n",
    "dtc.fit(X_train, y_train)\n",
    "pred = dtc.predict(X_test)\n",
    "# 모델 평가지표 출력\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=5, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "poly.transform(X_train)\n",
    "poly.fit_transform(X_train)\n",
    "poly.get_feature_names_out()\n",
    "pd.options.display.max_columns = 150\n",
    "# 데이터프레임으로 제작 후 데이터 확인\n",
    "poly_X_train = pd.DataFrame(poly.transform(X_train),columns=poly.get_feature_names_out())\n",
    "# test 데이터 동일한 모델로 적용\n",
    "poly_X_test = poly.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "# 모델 학습\n",
    "ls = Lasso(alpha=100)\n",
    "ls.fit(poly_X_train, y_train)\n",
    "# 모델 예측\n",
    "ls_pred = ls.predict(poly_X_test)\n",
    "# 모델 평가\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(r2_score(y_test, ls_pred))   # 정답에 해당하는 y_test 먼저\n",
    "print(mean_squared_error(y_test, ls_pred, squared=False))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc =  DecisionTreeClassifier(max_depth=15, random_state=42)\n",
    "dtc.fit(X_train,y_train)\n",
    "pred = dtc.predict(X_test)\n",
    "# 예측 확률값 확인\n",
    "# CODE HERE\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kospi = pd.read_csv('./data/kospi.csv', encoding='cp949')\n",
    "X = kospi.drop('코스피지수', axis=1)\n",
    "y = kospi['코스피지수']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "from xgboost import XGBRegressor\n",
    "xg = XGBRegressor()\n",
    "xg.fit(X_train, y_train)\n",
    "pred = xg.predict(X_test)\n",
    "# 평가지표 출력\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "depth = [5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "est = [80, 100, 300, 500, 700, 900]\n",
    "# 검증셋 validation set 생성\n",
    "best_score = 0\n",
    "best_param = 0\n",
    "for param in list(product(depth, est)):\n",
    "    print(f'{param} 서칭중')\n",
    "    model = RandomForestRegressor(n_estimators=param[1], max_depth=param[0], random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)   #파라미터 서칭을 진행하며 사용하는 검증셋을 validation셋 사용\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    if r2 > best_score : \n",
    "        print('스코어 갱신')\n",
    "        best_param = param \n",
    "        best_score = r2\n",
    "print('학습완료')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score, best_param\n",
    "# 최적 모델로 모델 다시 학습 및 평가\n",
    "best_model = RandomForestRegressor(n_estimators=3300,max_depth=11)\n",
    "best_model.fit(X_train, y_train)\n",
    "best_pred = best_model.predict(X_test)\n",
    "# 모델 평가1 예측\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f'R2 score: {r2_score(y_test, best_pred)}')   # 정답에 해당하는 y_test 먼저\n",
    "print(f'RMSE:{mean_squared_error(y_test, best_pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "# 모델 정의\n",
    "# 규제화모델의 경우 파라메터로 규제화강도에 해당하는 alpha 값을 같이 전달함. \n",
    "rg = Ridge(alpha=7)\n",
    "# 모델 학습\n",
    "rg.fit(X_train, y_train)\n",
    "# 모델 예측\n",
    "rg_pred = rg.predict(X_test)\n",
    "# 모델 평가\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(r2_score(y_test, rg_pred))\n",
    "print(mean_squared_error(y_test, rg_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kospi = pd.read_csv('./data/kospi.csv', encoding='cp949')\n",
    "X = kospi.drop('코스피지수', axis=1)\n",
    "y = kospi['코스피지수']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "params = {\n",
    "'max_depth' : [15,17,19,21],\n",
    "'n_estimators' : [50,70,100,200],\n",
    "'learning_rate' : [0.01, 0.03],\n",
    "'subsample' : [0.7,0.8,0.9],\n",
    "'colsample_bytree' : [0.7,0.8,0.9]\n",
    "}\n",
    "model = XGBRegressor(random_state=42)\n",
    "grid = Gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kospi = pd.read_csv('./data/kospi.csv', encoding='cp949')\n",
    "X = kospi.drop('코스피지수', axis=1)\n",
    "y = kospi['코스피지수']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train =abs(y_train -1)\n",
    "y_test = abs(y_test -1)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "xgbc = XGBClassifier(random_state=42)\n",
    "params2 = {'max_depth' : [3, 5, 7, 9],\n",
    "        'n_estimators' : [100,200],\n",
    "        'learning_rate' : [0.03, 0.01],\n",
    "        'subsample' : [0.7,0.8,0.9],\n",
    "        'colsample_bytree' : [0.7, 0.8, 0.9]}\n",
    "\n",
    "grid2 = GridSearchCV(estimator=xgbc, param_grid=params2, scoring='recall', verbose=2, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2_pred = grid2.predict(X_test)\n",
    "print(confusion_matrix(y_test, grid2_pred))\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(r2_score(y_test, grid2_pred))\n",
    "print(mean_squared_error(y_test, grid2_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9475143308110816\n",
      "103.28490627693634\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "xgbr = XGBRegressor(random_state=42)\n",
    "params = {'max_depth' : [15, 17, 19, 20],\n",
    "        'n_estimators' : [50,70,100,200],\n",
    "        'learning_rate' : [0.03,0.01],\n",
    "        'subsample' : [0.7,0.8,0.9],\n",
    "        'colsample_bytree' : [0.7, 0.8, 0.9]}\n",
    "grid = GridSearchCV(estimator=xgbr,\n",
    "                    param_grid=params,\n",
    "                    scoring='neg_root_mean_squared_error',\n",
    "                    n_jobs=-1,\n",
    "                    cv=5\n",
    "                    )\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "grid_pred = grid.predict(X_test)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(r2_score(y_test, grid_pred))\n",
    "print(f'{mean_squared_error(y_test, grid_pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.03, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=100, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=9, n_estimators=200, subsample=0.9; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "720 fits failed out of a total of 720.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "144 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1467, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87], got [ 669.9  735.3  759.5  782.4  785.8  810.7  834.8  848.5  911.3  932.7\n",
      " 1008.2 1011.4 1206.3 1221.  1295.2 1297.8 1352.7 1359.6 1360.2 1369.4\n",
      " 1371.4 1371.6 1390.1 1395.9 1419.7 1434.5 1448.1 1452.6 1474.2 1542.2\n",
      " 1557.3 1580.7 1594.7 1602.4 1674.9 1682.8 1700.9 1704.  1711.6 1741.6\n",
      " 1742.8 1743.6 1759.3 1825.5 1825.7 1847.5 1863.3 1872.8 1880.1 1882.\n",
      " 1883.  1905.1 1906.  1912.1 1914.  1916.7 1933.3 1939.3 1946.5 1949.3\n",
      " 1961.9 1962.8 1964.4 1970.4 1980.  1982.  1983.4 1985.6 1992.  1994.2\n",
      " 1995.9 1996.2 1997.  2001.1 2002.2 2004.9 2014.  2016.2 2026.5 2029.5\n",
      " 2030.1 2034.7 2051.  2064.9 2069.7 2074.2 2076.1 2114.8]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "144 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1467, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88], got [ 633.4  669.9  697.5  735.3  759.5  782.4  785.8  796.2  810.7  834.8\n",
      "  848.5  862.8  880.5  932.7 1011.4 1083.3 1113.1 1124.5 1162.1 1206.3\n",
      " 1295.2 1297.8 1359.6 1364.6 1369.4 1395.9 1399.8 1419.7 1434.5 1452.6\n",
      " 1474.2 1557.3 1580.7 1594.7 1602.4 1673.1 1698.3 1700.9 1704.  1711.6\n",
      " 1741.6 1742.8 1743.6 1825.5 1825.7 1847.5 1854.  1863.3 1872.8 1880.1\n",
      " 1882.  1883.  1906.  1912.1 1914.  1915.6 1916.7 1932.9 1933.3 1946.5\n",
      " 1949.3 1961.9 1962.8 1964.  1964.4 1970.4 1980.  1982.  1985.6 1992.\n",
      " 1995.  1995.9 1997.  2002.2 2011.3 2014.  2016.2 2029.5 2030.1 2034.7\n",
      " 2044.9 2051.  2064.9 2068.5 2069.7 2074.2 2114.8 2127.2 2192.4]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "144 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1467, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88], got [ 633.4  669.9  697.5  735.3  759.5  785.8  796.2  834.8  848.5  862.8\n",
      "  880.5  911.3  932.7 1008.2 1011.4 1083.3 1113.1 1124.5 1162.1 1221.\n",
      " 1295.2 1297.8 1352.7 1360.2 1364.6 1369.4 1371.4 1371.6 1390.1 1395.9\n",
      " 1399.8 1448.1 1474.2 1542.2 1557.3 1673.1 1674.9 1682.8 1698.3 1700.9\n",
      " 1704.  1711.6 1742.8 1759.3 1825.5 1825.7 1847.5 1854.  1863.3 1872.8\n",
      " 1880.1 1883.  1905.1 1906.  1912.1 1915.6 1932.9 1933.3 1939.3 1946.5\n",
      " 1949.3 1961.9 1964.  1964.4 1970.4 1980.  1982.  1983.4 1985.6 1992.\n",
      " 1994.2 1995.  1995.9 1996.2 2001.1 2002.2 2004.9 2011.3 2014.  2026.5\n",
      " 2034.7 2044.9 2068.5 2069.7 2074.2 2076.1 2114.8 2127.2 2192.4]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "144 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1467, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89], got [ 633.4  669.9  697.5  735.3  782.4  796.2  810.7  862.8  880.5  911.3\n",
      "  932.7 1008.2 1011.4 1083.3 1113.1 1124.5 1162.1 1206.3 1221.  1295.2\n",
      " 1352.7 1359.6 1360.2 1364.6 1369.4 1371.4 1371.6 1390.1 1399.8 1419.7\n",
      " 1434.5 1448.1 1452.6 1542.2 1557.3 1580.7 1594.7 1602.4 1673.1 1674.9\n",
      " 1682.8 1698.3 1711.6 1741.6 1742.8 1743.6 1759.3 1825.7 1847.5 1854.\n",
      " 1863.3 1880.1 1882.  1905.1 1912.1 1914.  1915.6 1916.7 1932.9 1939.3\n",
      " 1946.5 1961.9 1962.8 1964.  1980.  1983.4 1985.6 1992.  1994.2 1995.\n",
      " 1995.9 1996.2 1997.  2001.1 2004.9 2011.3 2014.  2016.2 2026.5 2029.5\n",
      " 2030.1 2034.7 2044.9 2051.  2064.9 2068.5 2069.7 2076.1 2127.2 2192.4]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "144 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 1467, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88], got [ 633.4  697.5  759.5  782.4  785.8  796.2  810.7  834.8  848.5  862.8\n",
      "  880.5  911.3 1008.2 1083.3 1113.1 1124.5 1162.1 1206.3 1221.  1297.8\n",
      " 1352.7 1359.6 1360.2 1364.6 1371.4 1371.6 1390.1 1395.9 1399.8 1419.7\n",
      " 1434.5 1448.1 1452.6 1474.2 1542.2 1580.7 1594.7 1602.4 1673.1 1674.9\n",
      " 1682.8 1698.3 1700.9 1704.  1741.6 1743.6 1759.3 1825.5 1854.  1872.8\n",
      " 1882.  1883.  1905.1 1906.  1912.1 1914.  1915.6 1916.7 1932.9 1933.3\n",
      " 1939.3 1949.3 1962.8 1964.  1964.4 1970.4 1982.  1983.4 1994.2 1995.\n",
      " 1996.2 1997.  2001.1 2002.2 2004.9 2011.3 2016.2 2026.5 2029.5 2030.1\n",
      " 2044.9 2051.  2064.9 2068.5 2074.2 2076.1 2114.8 2127.2 2192.4]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n 108 109 110], got [ 633.4  669.9  697.5  735.3  759.5  782.4  785.8  796.2  810.7  834.8\n  848.5  862.8  880.5  911.3  932.7 1008.2 1011.4 1083.3 1113.1 1124.5\n 1162.1 1206.3 1221.  1295.2 1297.8 1352.7 1359.6 1360.2 1364.6 1369.4\n 1371.4 1371.6 1390.1 1395.9 1399.8 1419.7 1434.5 1448.1 1452.6 1474.2\n 1542.2 1557.3 1580.7 1594.7 1602.4 1673.1 1674.9 1682.8 1698.3 1700.9\n 1704.  1711.6 1741.6 1742.8 1743.6 1759.3 1825.5 1825.7 1847.5 1854.\n 1863.3 1872.8 1880.1 1882.  1883.  1905.1 1906.  1912.1 1914.  1915.6\n 1916.7 1932.9 1933.3 1939.3 1946.5 1949.3 1961.9 1962.8 1964.  1964.4\n 1970.4 1980.  1982.  1983.4 1985.6 1992.  1994.2 1995.  1995.9 1996.2\n 1997.  2001.1 2002.2 2004.9 2011.3 2014.  2016.2 2026.5 2029.5 2030.1\n 2034.7 2044.9 2051.  2064.9 2068.5 2069.7 2074.2 2076.1 2114.8 2127.2\n 2192.4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12300\\4018791450.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mgrid2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgbc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'recall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mgrid2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mgrid2_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1465\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mexpected_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m             ):\n\u001b[1;32m-> 1467\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   1468\u001b[0m                     \u001b[1;34mf\"Invalid classes inferred from unique values of `y`.  \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m                     \u001b[1;34mf\"Expected: {expected_classes}, got {classes}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n 108 109 110], got [ 633.4  669.9  697.5  735.3  759.5  782.4  785.8  796.2  810.7  834.8\n  848.5  862.8  880.5  911.3  932.7 1008.2 1011.4 1083.3 1113.1 1124.5\n 1162.1 1206.3 1221.  1295.2 1297.8 1352.7 1359.6 1360.2 1364.6 1369.4\n 1371.4 1371.6 1390.1 1395.9 1399.8 1419.7 1434.5 1448.1 1452.6 1474.2\n 1542.2 1557.3 1580.7 1594.7 1602.4 1673.1 1674.9 1682.8 1698.3 1700.9\n 1704.  1711.6 1741.6 1742.8 1743.6 1759.3 1825.5 1825.7 1847.5 1854.\n 1863.3 1872.8 1880.1 1882.  1883.  1905.1 1906.  1912.1 1914.  1915.6\n 1916.7 1932.9 1933.3 1939.3 1946.5 1949.3 1961.9 1962.8 1964.  1964.4\n 1970.4 1980.  1982.  1983.4 1985.6 1992.  1994.2 1995.  1995.9 1996.2\n 1997.  2001.1 2002.2 2004.9 2011.3 2014.  2016.2 2026.5 2029.5 2030.1\n 2034.7 2044.9 2051.  2064.9 2068.5 2069.7 2074.2 2076.1 2114.8 2127.2\n 2192.4]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "kospi = pd.read_csv('./data/kospi.csv', encoding='cp949')\n",
    "X = kospi.drop('코스피지수', axis=1)\n",
    "y = kospi['코스피지수']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "xgbc = XGBClassifier(random_state=42)\n",
    "params2 = {'max_depth' : [3, 5, 7, 9],\n",
    "        'n_estimators' : [100,200],\n",
    "        'learning_rate' : [0.03, 0.01],\n",
    "        'subsample' : [0.7,0.8,0.9],\n",
    "        'colsample_bytree' : [0.7, 0.8, 0.9]}\n",
    "grid2 = GridSearchCV(estimator=xgbc, param_grid=params2, scoring='recall', verbose=2, cv=5)\n",
    "\n",
    "grid2.fit(X_train, y_train)\n",
    "grid2_pred = grid2.predict(X_test)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(r2_score(y_test, grid2_pred))\n",
    "print(f'{mean_squared_error(y_test, grid2_pred, squared=False)}')  ## squared가 폴스면 루트 씌우는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test_size=0.3, random_state=42\n",
    "## RMSE 기준 소수점 네자리 가장 작은 스코어를 만들어내기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
